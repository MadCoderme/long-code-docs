{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9538295,"sourceType":"datasetVersion","datasetId":5809957},{"sourceId":9538301,"sourceType":"datasetVersion","datasetId":5809962},{"sourceId":9540945,"sourceType":"datasetVersion","datasetId":5812013}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/abrarfairujraiyan/overcoming-output-token-limitation-for-docs-genera?scriptVersionId=199375159\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install -q -U google-generativeai","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:08:22.891854Z","iopub.execute_input":"2024-10-03T10:08:22.892307Z","iopub.status.idle":"2024-10-03T10:08:55.119964Z","shell.execute_reply.started":"2024-10-03T10:08:22.892253Z","shell.execute_reply":"2024-10-03T10:08:55.118491Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Configure the library and start a chat with the model","metadata":{}},{"cell_type":"code","source":"import os\nimport google.generativeai as genai\nfrom google.generativeai import caching\nimport datetime\nimport time\n\ngenai.configure(api_key='YOUR_API_KEY')\n\nfile = open('/kaggle/input/sample-long-code/trainer.py', 'r')\ncode = file.read()\n\nmodel = genai.GenerativeModel('gemini-1.5-flash')\nchat = model.start_chat(history=[])\n\ncombined_doc = ''\n\nresponse = chat.send_message(f\"\"\"Code:{code}\n\nYou need to generate documentation for this code. Since this file is large, you will first generate the table of content. Later, you will generate documentation for the first few contents/titles from the table. Then, generate next few and next few and this will continue until whole table of content is covered.\nDocumentation requirement: Explain the parameters and algorithms for the functions\n\nStart by generating the table of content only\"\"\")\ncombined_doc = response.text\nprint(response.text)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-03T17:58:30.522281Z","iopub.execute_input":"2024-10-03T17:58:30.522757Z","iopub.status.idle":"2024-10-03T17:58:34.439959Z","shell.execute_reply.started":"2024-10-03T17:58:30.522711Z","shell.execute_reply":"2024-10-03T17:58:34.438805Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"## Table of Contents\n\n1. **BaseTrainer Class**\n    - `__init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None)`\n    - `add_callback(self, event: str, callback)`\n    - `set_callback(self, event: str, callback)`\n    - `run_callbacks(self, event: str)`\n    - `train(self)`\n    - `_setup_scheduler(self)`\n    - `_setup_ddp(self, world_size)`\n    - `_setup_train(self, world_size)`\n    - `_do_train(self, world_size=1)`\n    - `_get_memory(self)`\n    - `_clear_memory(self)`\n    - `read_results_csv(self)`\n    - `save_model(self)`\n    - `get_dataset(self)`\n    - `setup_model(self)`\n    - `optimizer_step(self)`\n    - `preprocess_batch(self, batch)`\n    - `validate(self)`\n    - `get_model(self, cfg=None, weights=None, verbose=True)`\n    - `get_validator(self)`\n    - `get_dataloader(self, dataset_path, batch_size=16, rank=0, mode=\"train\")`\n    - `build_dataset(self, img_path, mode=\"train\", batch=None)`\n    - `label_loss_items(self, loss_items=None, prefix=\"train\")`\n    - `set_model_attributes(self)`\n    - `build_targets(self, preds, targets)`\n    - `progress_string(self)`\n    - `plot_training_samples(self, batch, ni)`\n    - `plot_training_labels(self)`\n    - `save_metrics(self, metrics)`\n    - `plot_metrics(self)`\n    - `on_plot(self, name, data=None)`\n    - `final_eval(self)`\n    - `check_resume(self, overrides)`\n    - `resume_training(self, ckpt)`\n    - `_close_dataloader_mosaic(self)`\n    - `build_optimizer(self, model, name=\"auto\", lr=0.001, momentum=0.9, decay=1e-5, iterations=1e5)`\n\n2. **Functions**\n    - `check_model_file_from_stem(model)`\n    - `print_args(args)`\n    - `check_amp(model)`\n    - `check_file(file)`\n    - `check_imgsz(imgsz, stride, floor, max_dim)`\n    - `generate_ddp_command(world_size, trainer)`\n    - `ddp_cleanup(trainer, file)`\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This table of content is not usable in code. So, turn it into a json array","metadata":{}},{"cell_type":"code","source":"import json\n\njson_resp = chat.send_message(\"Now give me the table of content as a flat json array. \")\n\njson_string = json_resp.text.replace('```json', '').replace('```', '').strip()\ndata = json.loads(json_string)\nprint(data)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:59:42.847758Z","iopub.execute_input":"2024-10-03T17:59:42.848166Z","iopub.status.idle":"2024-10-03T17:59:46.160842Z","shell.execute_reply.started":"2024-10-03T17:59:42.848125Z","shell.execute_reply":"2024-10-03T17:59:46.159128Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"['BaseTrainer Class', '__init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None)', 'add_callback(self, event: str, callback)', 'set_callback(self, event: str, callback)', 'run_callbacks(self, event: str)', 'train(self)', '_setup_scheduler(self)', '_setup_ddp(self, world_size)', '_setup_train(self, world_size)', '_do_train(self, world_size=1)', '_get_memory(self)', '_clear_memory(self)', 'read_results_csv(self)', 'save_model(self)', 'get_dataset(self)', 'setup_model(self)', 'optimizer_step(self)', 'preprocess_batch(self, batch)', 'validate(self)', 'get_model(self, cfg=None, weights=None, verbose=True)', 'get_validator(self)', 'get_dataloader(self, dataset_path, batch_size=16, rank=0, mode=\"train\")', 'build_dataset(self, img_path, mode=\"train\", batch=None)', 'label_loss_items(self, loss_items=None, prefix=\"train\")', 'set_model_attributes(self)', 'build_targets(self, preds, targets)', 'progress_string(self)', 'plot_training_samples(self, batch, ni)', 'plot_training_labels(self)', 'save_metrics(self, metrics)', 'plot_metrics(self)', 'on_plot(self, name, data=None)', 'final_eval(self)', 'check_resume(self, overrides)', 'resume_training(self, ckpt)', '_close_dataloader_mosaic(self)', 'build_optimizer(self, model, name=\"auto\", lr=0.001, momentum=0.9, decay=1e-5, iterations=1e5)', 'Functions', 'check_model_file_from_stem(model)', 'print_args(args)', 'check_amp(model)', 'check_file(file)', 'check_imgsz(imgsz, stride, floor, max_dim)', 'generate_ddp_command(world_size, trainer)', 'ddp_cleanup(trainer, file)']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Divide the list into multiple groups. The initial structure is not important.","metadata":{}},{"cell_type":"code","source":"divided_table = [data[i:i + 10] for i in range(0, len(data), 10)]\n\nprint(divided_table)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:59:55.340192Z","iopub.execute_input":"2024-10-03T17:59:55.34066Z","iopub.status.idle":"2024-10-03T17:59:55.347613Z","shell.execute_reply.started":"2024-10-03T17:59:55.340614Z","shell.execute_reply":"2024-10-03T17:59:55.346262Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[['BaseTrainer Class', '__init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None)', 'add_callback(self, event: str, callback)', 'set_callback(self, event: str, callback)', 'run_callbacks(self, event: str)', 'train(self)', '_setup_scheduler(self)', '_setup_ddp(self, world_size)', '_setup_train(self, world_size)', '_do_train(self, world_size=1)'], ['_get_memory(self)', '_clear_memory(self)', 'read_results_csv(self)', 'save_model(self)', 'get_dataset(self)', 'setup_model(self)', 'optimizer_step(self)', 'preprocess_batch(self, batch)', 'validate(self)', 'get_model(self, cfg=None, weights=None, verbose=True)'], ['get_validator(self)', 'get_dataloader(self, dataset_path, batch_size=16, rank=0, mode=\"train\")', 'build_dataset(self, img_path, mode=\"train\", batch=None)', 'label_loss_items(self, loss_items=None, prefix=\"train\")', 'set_model_attributes(self)', 'build_targets(self, preds, targets)', 'progress_string(self)', 'plot_training_samples(self, batch, ni)', 'plot_training_labels(self)', 'save_metrics(self, metrics)'], ['plot_metrics(self)', 'on_plot(self, name, data=None)', 'final_eval(self)', 'check_resume(self, overrides)', 'resume_training(self, ckpt)', '_close_dataloader_mosaic(self)', 'build_optimizer(self, model, name=\"auto\", lr=0.001, momentum=0.9, decay=1e-5, iterations=1e5)', 'Functions', 'check_model_file_from_stem(model)', 'print_args(args)'], ['check_amp(model)', 'check_file(file)', 'check_imgsz(imgsz, stride, floor, max_dim)', 'generate_ddp_command(world_size, trainer)', 'ddp_cleanup(trainer, file)']]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Generate documentation part by part and merge them all together.","metadata":{}},{"cell_type":"code","source":"partial_resp = chat.send_message('Generate short documentation for the first 10 items in the table of content. Don\\'t repeat the title')\n# print(partial_resp.text)\ncombined_doc += partial_resp.text\ndivided_table.pop(0)\n\nfor i in divided_table:\n    partial_resp = chat.send_message('Generate short documentation for the next ' + str(len(i)) + ' items in the table of content. Don\\'t repeat the title')\n    combined_doc += partial_resp.text\n    # print(partial_resp.text)\n\nprint(combined_doc)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T18:00:07.519007Z","iopub.execute_input":"2024-10-03T18:00:07.520216Z","iopub.status.idle":"2024-10-03T18:00:20.171247Z","shell.execute_reply.started":"2024-10-03T18:00:07.520147Z","shell.execute_reply":"2024-10-03T18:00:20.170085Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"## Table of Contents\n\n1. **BaseTrainer Class**\n    - `__init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None)`\n    - `add_callback(self, event: str, callback)`\n    - `set_callback(self, event: str, callback)`\n    - `run_callbacks(self, event: str)`\n    - `train(self)`\n    - `_setup_scheduler(self)`\n    - `_setup_ddp(self, world_size)`\n    - `_setup_train(self, world_size)`\n    - `_do_train(self, world_size=1)`\n    - `_get_memory(self)`\n    - `_clear_memory(self)`\n    - `read_results_csv(self)`\n    - `save_model(self)`\n    - `get_dataset(self)`\n    - `setup_model(self)`\n    - `optimizer_step(self)`\n    - `preprocess_batch(self, batch)`\n    - `validate(self)`\n    - `get_model(self, cfg=None, weights=None, verbose=True)`\n    - `get_validator(self)`\n    - `get_dataloader(self, dataset_path, batch_size=16, rank=0, mode=\"train\")`\n    - `build_dataset(self, img_path, mode=\"train\", batch=None)`\n    - `label_loss_items(self, loss_items=None, prefix=\"train\")`\n    - `set_model_attributes(self)`\n    - `build_targets(self, preds, targets)`\n    - `progress_string(self)`\n    - `plot_training_samples(self, batch, ni)`\n    - `plot_training_labels(self)`\n    - `save_metrics(self, metrics)`\n    - `plot_metrics(self)`\n    - `on_plot(self, name, data=None)`\n    - `final_eval(self)`\n    - `check_resume(self, overrides)`\n    - `resume_training(self, ckpt)`\n    - `_close_dataloader_mosaic(self)`\n    - `build_optimizer(self, model, name=\"auto\", lr=0.001, momentum=0.9, decay=1e-5, iterations=1e5)`\n\n2. **Functions**\n    - `check_model_file_from_stem(model)`\n    - `print_args(args)`\n    - `check_amp(model)`\n    - `check_file(file)`\n    - `check_imgsz(imgsz, stride, floor, max_dim)`\n    - `generate_ddp_command(world_size, trainer)`\n    - `ddp_cleanup(trainer, file)`\n\n```python\n\"\"\"\nInitializes the BaseTrainer class.\n\nArgs:\n    cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.\n    overrides (dict, optional): Configuration overrides. Defaults to None.\n\"\"\"\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n    pass\n\n\"\"\"\nAppends the given callback.\n\"\"\"\ndef add_callback(self, event: str, callback):\n    pass\n\n\"\"\"\nOverrides the existing callbacks with the given callback.\n\"\"\"\ndef set_callback(self, event: str, callback):\n    pass\n\n\"\"\"\nRun all existing callbacks associated with a particular event.\n\"\"\"\ndef run_callbacks(self, event: str):\n    pass\n\n\"\"\"\nAllow device='', device=None on Multi-GPU systems to default to device=0.\n\"\"\"\ndef train(self):\n    pass\n\n\"\"\"\nInitialize training learning rate scheduler.\n\"\"\"\ndef _setup_scheduler(self):\n    pass\n\n\"\"\"\nInitializes and sets the DistributedDataParallel parameters for training.\n\"\"\"\ndef _setup_ddp(self, world_size):\n    pass\n\n\"\"\"\nBuilds dataloaders and optimizer on correct rank process.\n\"\"\"\ndef _setup_train(self, world_size):\n    pass\n\n\"\"\"\nTrain completed, evaluate and plot if specified by arguments.\n\"\"\"\ndef _do_train(self, world_size=1):\n    pass\n\n\"\"\"\nGet accelerator memory utilization in GB.\n\"\"\"\ndef _get_memory(self):\n    pass\n``````python\n\"\"\"\nClear accelerator memory on different platforms.\n\"\"\"\ndef _clear_memory(self):\n    pass\n\n\"\"\"\nRead results.csv into a dict using pandas.\n\"\"\"\ndef read_results_csv(self):\n    pass\n\n\"\"\"\nSave model training checkpoints with additional metadata.\n\"\"\"\ndef save_model(self):\n    pass\n\n\"\"\"\nGet train, val path from data dict if it exists.\n\nReturns None if data format is not recognized.\n\"\"\"\ndef get_dataset(self):\n    pass\n\n\"\"\"\nLoad/create/download model for any task.\n\"\"\"\ndef setup_model(self):\n    pass\n\n\"\"\"\nPerform a single step of the training optimizer with gradient clipping and EMA update.\n\"\"\"\ndef optimizer_step(self):\n    pass\n\n\"\"\"\nAllows custom preprocessing model inputs and ground truths depending on task type.\n\"\"\"\ndef preprocess_batch(self, batch):\n    pass\n\n\"\"\"\nRuns validation on test set using self.validator.\n\nThe returned dict is expected to contain \"fitness\" key.\n\"\"\"\ndef validate(self):\n    pass\n\n\"\"\"\nGet model and raise NotImplementedError for loading cfg files.\n\"\"\"\ndef get_model(self, cfg=None, weights=None, verbose=True):\n    pass\n```\n\n```python\n\"\"\"\nReturns a NotImplementedError when the get_validator function is called.\n\"\"\"\ndef get_validator(self):\n    pass\n\n\"\"\"\nReturns dataloader derived from torch.data.Dataloader.\n\"\"\"\ndef get_dataloader(self, dataset_path, batch_size=16, rank=0, mode=\"train\"):\n    pass\n\n\"\"\"\nBuild dataset.\n\"\"\"\ndef build_dataset(self, img_path, mode=\"train\", batch=None):\n    pass\n\n\"\"\"\nReturns a loss dict with labelled training loss items tensor.\n\nNote:\n    This is not needed for classification but necessary for segmentation & detection\n\"\"\"\ndef label_loss_items(self, loss_items=None, prefix=\"train\"):\n    pass\n\n\"\"\"\nTo set or update model parameters before training.\n\"\"\"\ndef set_model_attributes(self):\n    pass\n\n\"\"\"\nBuilds target tensors for training YOLO model.\n\"\"\"\ndef build_targets(self, preds, targets):\n    pass\n\n\"\"\"\nReturns a string describing training progress.\n\"\"\"\ndef progress_string(self):\n    pass\n\n\"\"\"\nPlots training samples during YOLO training.\n\"\"\"\ndef plot_training_samples(self, batch, ni):\n    pass\n\n\"\"\"\nPlots training labels for YOLO model.\n\"\"\"\ndef plot_training_labels(self):\n    pass\n```\n\n```python\n\"\"\"\nSaves training metrics to a CSV file.\n\"\"\"\ndef save_metrics(self, metrics):\n    pass\n\n\"\"\"\nPlot and display metrics visually.\n\"\"\"\ndef plot_metrics(self):\n    pass\n\n\"\"\"\nRegisters plots (e.g. to be consumed in callbacks).\n\"\"\"\ndef on_plot(self, name, data=None):\n    pass\n\n\"\"\"\nPerforms final evaluation and validation for object detection YOLO model.\n\"\"\"\ndef final_eval(self):\n    pass\n\n\"\"\"\nCheck if resume checkpoint exists and update arguments accordingly.\n\"\"\"\ndef check_resume(self, overrides):\n    pass\n\n\"\"\"\nResume YOLO training from given epoch and best fitness.\n\"\"\"\ndef resume_training(self, ckpt):\n    pass\n\n\"\"\"\nUpdate dataloaders to stop using mosaic augmentation.\n\"\"\"\ndef _close_dataloader_mosaic(self):\n    pass\n\n\"\"\"\nConstructs an optimizer for the given model, based on the specified optimizer name, learning rate, momentum,\nweight decay, and number of iterations.\n\nArgs:\n    model (torch.nn.Module): The model for which to build an optimizer.\n    name (str, optional): The name of the optimizer to use. If 'auto', the optimizer is selected\n        based on the number of iterations. Default: 'auto'.\n    lr (float, optional): The learning rate for the optimizer. Default: 0.001.\n    momentum (float, optional): The momentum factor for the optimizer. Default: 0.9.\n    decay (float, optional): The weight decay for the optimizer. Default: 1e-5.\n    iterations (float, optional): The number of iterations, which determines the optimizer if\n        name is 'auto'. Default: 1e5.\n\nReturns:\n    (torch.optim.Optimizer): The constructed optimizer.\n\"\"\"\ndef build_optimizer(self, model, name=\"auto\", lr=0.001, momentum=0.9, decay=1e-5, iterations=1e5):\n    pass\n```\n\n```python\n\"\"\"\nAdd suffix, i.e. yolov8n -> yolov8n.pt\n\"\"\"\ndef check_model_file_from_stem(model):\n    pass\n\n\"\"\"\nPrint arguments for debugging.\n\"\"\"\ndef print_args(args):\n    pass\n\n\"\"\"\nCheck if Automatic Mixed Precision (AMP) is supported and enabled.\n\"\"\"\ndef check_amp(model):\n    pass\n\n\"\"\"\nCheck if the given file exists.\n\"\"\"\ndef check_file(file):\n    pass\n\n\"\"\"\nCheck and adjust image size (imgsz) for training.\n\"\"\"\ndef check_imgsz(imgsz, stride, floor, max_dim):\n    pass\n```\n","output_type":"stream"}]}]}
